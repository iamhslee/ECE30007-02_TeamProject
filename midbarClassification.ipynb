{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI프로젝트입문 (ECE30007-02)\n",
    "Spring, 2023\n",
    "\n",
    "Team Project: 미드바르 뿌리 이미지 인식 챌린지\n",
    "\n",
    "`midbarClassification.py`\n",
    "\n",
    "Author: 이현서 (ID#22100600) <hslee@handong.ac.kr>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZES = 4\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "CLASSES = [\"0012\", \"1224\", \"2436\", \"3648\", \"4860\", \"6072\", \"OVER\"]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "DATASET_PATH = \"./Dataset\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"train\")\n",
    "TEST_PATH = os.path.join(DATASET_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class midbarDataset(data.Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.image_paths, self.labels = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "\n",
    "        for class_name in CLASSES:\n",
    "            class_dir = os.path.join(self.data_path, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                class_images = [img for img in os.listdir(class_dir) if img.endswith(\".png\")]\n",
    "                image_paths.extend([os.path.join(class_dir, img) for img in class_images])\n",
    "                labels.extend([class_name] * len(class_images))\n",
    "\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        image = self.load_image(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, CLASSES.index(label)\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        image = image.resize((128, 128))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = midbarDataset(TRAIN_PATH, transform=train_transform)\n",
    "test_dataset = midbarDataset(TEST_PATH, transform=test_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZES, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZES, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU support : False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"GPU support : \" + (\"True\" if torch.cuda.is_available() else \"False\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = Net().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3q/v_1rx_t54hq7jy00p987_tjw0000gn/T/ipykernel_36176/1366270942.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs, labels = data[0].to(device), torch.tensor(data[1]).to(device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Net.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs, labels \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[39m=\u001b[39m Net(inputs)\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m      9\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mTypeError\u001b[0m: Net.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), torch.tensor(data[1]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = Net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "torch.save(net.state_dict(), \"./midbar.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = Net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Test Accuracy: {accuracy}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
